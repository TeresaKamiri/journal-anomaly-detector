# Journal Anomaly Detector

This project aims to detect anomalies in journal entries combining autoencoder-based unsupervised learning with explainability (SHAP) and benchmarking (Random Forest, KNN). The system identifies unusual patterns in journal entries, helping auditors spot potential errors, fraud, or irregularities.

## Project Structure

```bash
journal-anomaly-detector/
â”œâ”€â”€ data/                                # Data files
â”‚   â”œâ”€â”€ synthetic_versions/            # Synthetic Raw journal entries dataset
â”‚   â”‚   â”œâ”€â”€ synthetic_labeled_v1.csv   
â”‚   â”‚   â”œâ”€â”€ synthetic_labeled_v2.csv
â”‚   â”‚   â”œâ”€â”€ synthetic_labeled_v3.csv
â”‚   â”‚   â”œâ”€â”€ synthetic_labeled_v4.csv
â”‚   â”‚   â”œâ”€â”€ synthetic_labeled_v5.csv
â”‚   â”œâ”€â”€ journal_entries.csv             # Raw journal entries dataset
â”œâ”€â”€ anomalies/                           # Anomalies detection results
â”‚   â”œâ”€â”€ anomalies_found.csv             # Detected anomalies
â”œâ”€â”€ evalutations/                       # Benchmark evaluation results
â”œâ”€â”€ models/                              # Saved models and preprocessing artifacts
â”‚   â”œâ”€â”€ autoencoder_model.keras        # Trained autoencoder model (Keras format)
â”‚   â”œâ”€â”€ feature_names.pkl              # List of features used in the model
â”‚   â”œâ”€â”€ scaler.pkl                     # Scaler used for data preprocessing
â”œâ”€â”€ notebooks/                           # Jupyter notebooks for exploratory analysis and preprocessing
â”‚   â”œâ”€â”€ eda_preprocessing.py           # Data exploration and preprocessing code
â”œâ”€â”€ src/                                 # Source code for anomaly detection and model training
â”‚   â”œâ”€â”€ benchmark_models.py            # Script for benchmarking model.  runs KNN/RF/Autoencoder comparison
â”‚   â”œâ”€â”€ detect_anomalies.py            # Script to detect anomalies in journal entries
â”‚   â”œâ”€â”€ explain_anomalies.py           # Script for explaining detected anomalies. uses SHAP for interpretation
â”‚   â”œâ”€â”€ generate_synthetic_journals.py # Script to generate synthetic journal entries for testing
â”‚   â”œâ”€â”€ preprocess.py                  # Script to preprocess and real datasets on retraining
â”‚   â”œâ”€â”€ render_anomalies.py            # Script to render anomaly data on dashboard
â”‚   â”œâ”€â”€ train_autoencoder.py           # Script to train the autoencoder model
â”‚   â”œâ”€â”€ train_on_upload.py             # Script to retrain on uploaded data
â”‚   â””â”€â”€ utils.py                       # Utility functions used across scripts
â”œâ”€â”€ dashboard/                           # Streamlit frontend for anomaly detection
â”‚   â””â”€â”€ app.py                         # Streamlit app for visualizing anomalies
â”œâ”€â”€ app.log                             # Log file to track app execution
â”œâ”€â”€ step_by_step.md                    # Step-by-step guide for project setup and usage
```

## Project Overview

This project uses an autoencoder-based deep learning model with explainability (SHAP) and benchmarking (Random Forest, KNN) to detect anomalies in journal entries. The model is trained on historical journal data, and the detected anomalies are flagged for further investigation.

### Main Features
- **Anomaly Detection:** Detects irregular patterns or outliers in journal entries using an autoencoder.
- **Explainability:** Uses explainability techniques to help auditors understand why certain entries are flagged as anomalies.
- **Synthetic Data Generation:** Generates synthetic journal entries for testing and model validation.
- **Optional: retrain model on uploaded data**
- **SHAP-based interpretability (force + summary plots)**
- **Benchmark:** comparison (Autoencoder vs. KNN vs. Random Forest), Supervised evaluation (if labels exist)
- **Streamlit Dashboard:** A frontend for visualizing and interacting with anomaly detection results.

## Requirements

You need the following libraries and tools to run this project:

- Python 3.10
- TensorFlow or Keras
- Scikit-learn
- Pandas
- Numpy
- Matplotlib
- Streamlit
- Other dependencies listed in `requirements.txt` (if provided)

## Setup Instructions

### Step 1: Create and Activate a Virtual Environment

1. Clone this repository to your local machine:

   ```bash
   git clone https://github.com/yourusername/journal-anomaly-detector.git
   ```

2. Navigate to the project directory:

   ```bash
   cd journal-anomaly-detector
   ```

3. Create a Python virtual environment:

   ```bash
   python3.10 -m venv ml-venv
   ```

4. Activate the virtual environment:

   - **On Windows:**
     ```bash
     .\ml-venv\Scripts\activate
     ```
   - **On macOS/Linux:**
     ```bash
     source ml-venv/bin/activate
     ```

### Step 2: Install the Required Dependencies

With the virtual environment activated, install the required Python dependencies:

```bash
pip install -r requirements.txt
```

### Step 3: Prepare Your ðŸ§ª Datasets

* Place labeled/unlabeled `.csv` files in `data/`
* If your dataset includes a `label` column (0 = normal, 1 = fraud), supervised evaluation + benchmarking will be activated

If needed, run the preprocessing and exploratory analysis steps in `notebooks/eda_preprocessing.py`.

## Usage 

### Generate Synthetic Data

You can generate synthetic journal entries using:

```bash
python src/generate_synthetic_journals.py
```

### Training the Model

1. Run the `train_autoencoder.py` script to train the autoencoder model:

   ```bash
   python src/train_autoencoder.py
   ```

2. After training, the model will be saved in the `models/` directory as `autoencoder_model.keras`.

### Detecting Anomalies

To detect anomalies in journal entries, run the `detect_anomalies.py` script:

```bash
python src/detect_anomalies.py
```

This will generate a CSV file (`anomalies_found.csv`) in the `anomalies/` folder containing all detected anomalies.

### Explaining Anomalies

To get explanations for why anomalies were detected, run:

```bash
python src/explain_anomalies.py
```
### Benchmark Model

KNN/RF/Autoencoder comparison, run:

```bash
python src/explain_anomalies.py
```

### Streamlit Dashboard

To start the Streamlit dashboard, use the following command:

```bash
streamlit run dashboard/app.py
```

This will launch a web application for visualizing detected anomalies, benchmarks, interacting with the dataset or retraining new datasets.

#### âœ… Hybrid Strategy
Default: Uses pretrained model on synthetic baseline
Optional: Retrain model on uploaded real data with checkbox toggle

#### ðŸ“¦ Outputs

* `evaluation/*.csv`: benchmark results
* `models/`: trained models + artifacts
* Downloadable anomaly report via UI

## Contributing

Feel free to fork this repository and submit pull requests for any improvements or fixes. If you have suggestions or issues, open an issue, and i will get back to you.
